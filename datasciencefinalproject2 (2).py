# -*- coding: utf-8 -*-
"""DataScienceFinalProject2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qEP1gmxwk41hodn7xMo5d9_4M_FVnuYP
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import kagglehub
import os

# Step 1
path = kagglehub.dataset_download("oktayrdeki/heart-disease")

print("Path to dataset files:", path)

files = os.listdir(path)
print("Files in dataset folder:", files)

# Assuming the main dataset is a CSV file (commonly `heart.csv` or similar)
csv_file = [f for f in files if f.endswith('heart_disease.csv')][0]
csv_path = os.path.join(path, csv_file)

# Load into a DataFrame
df = pd.read_csv(csv_path)

df.head(20)

num_entries = len(df)
print(f'The number of entries in the dataframe is: {num_entries}')

print(df.info())

print(df.head())

df.describe()

# Numeric columns—these will remain unchanged except for missing-value filling and scaling.
numeric_cols = [
    'Age', 'Blood Pressure', 'Cholesterol Level', 'BMI',
    'Sleep Hours', 'Triglyceride Level', 'Fasting Blood Sugar',
    'CRP Level', 'Homocysteine Level'
]

# Categorical columns—the raw column names that will be one-hot encoded.
categorical_cols = [
    'Gender', 'Exercise Habits', 'Smoking', 'Family Heart Disease',
    'Diabetes', 'High Blood Pressure', 'Low HDL Cholesterol',
    'High LDL Cholesterol', 'Alcohol Consumption', 'Stress Level',
    'Sugar Consumption', 'Heart Disease Status'
]

# For categorical columns, fill missing values with the mode
for col in categorical_cols:
    if col in df.columns:
        df[col] = df[col].fillna(df[col].mode()[0])

# For numeric columns, fill missing values with each column's median.
for col in numeric_cols:
    if col in df.columns:
        df[col] = df[col].fillna(df[col].median())

# Count duplicate rows
num_duplicates = df.duplicated().sum()
print(f"Number of duplicate rows: {num_duplicates}")

# Drop duplicate rows
df_cleaned = df.drop_duplicates()

# Verify the new shape
print(f"Shape before dropping duplicates: {df.shape}")
print(f"Shape after dropping duplicates:  {df_cleaned.shape}")

#Text Cleaning
for col in categorical_cols:
    if col in df.columns:
        df[col] = df[col].astype(str).str.strip().str.lower()

# Convert each categorical column to 'category' dtype.
for col in categorical_cols:
    if col in df.columns:
        df[col] = df[col].astype('category')

print("Columns currently in df:", df.columns.tolist())

# Standardize each numeric column: (value - mean) / standard deviation.
for col in numeric_cols:
    if col in df.columns:
        mean_val = df[col].mean()
        std_val = df[col].std()
        df[col] = (df[col] - mean_val) / std_val

print(df.columns)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import LabelEncoder

# Assuming 'Heart Disease Status' is the target column
X = df.drop('Heart Disease Status', axis=1)  # Changed 'target' to 'Heart Disease Status'
y = df['Heart Disease Status']  # Changed 'target' to 'Heart Disease Status'

# One-hot encode categorical features
X = pd.get_dummies(X, columns=categorical_cols[:-1]) # Exclude 'Heart Disease Status' from encoding

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of Logistic Regression: {accuracy:.2f}")

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# — you should already have: model, X_test, y_test, y_pred, and X (the DataFrame before scaling)
# If you overwrote X with the scaled array, just re-create your feature list:
feature_names = pd.get_dummies(df.drop('Heart Disease Status', axis=1),
                               columns=categorical_cols[:-1]).columns

# 1) ROC curve + AUC
y_prob = model.predict_proba(X_test)[:, 1]

# Specify 'yes' as the positive label
fpr, tpr, _ = roc_curve(y_test, y_prob, pos_label='yes')
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], '--', label='Random guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression ROC')
plt.legend(loc='lower right')
plt.show()

# 2) Coefficient bar chart
coefs = model.coef_.ravel()

plt.figure()
# horizontal bar so long names fit
plt.barh(feature_names, coefs)
plt.xlabel('Coefficient value')
plt.title('Logistic Regression Feature Importances')
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Use 'Heart Disease Status' instead of 'target'
X = df.drop('Heart Disease Status', axis=1)
y = df['Heart Disease Status']

# One-hot encode categorical features before scaling
X = pd.get_dummies(X, columns=categorical_cols[:-1]) # Exclude 'Heart Disease Status' from encoding

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of Decision Tree with DecisionTreeClassifier: {accuracy:.2f}")

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Get predicted probabilities for the positive class
y_prob = clf.predict_proba(X_test)[:, 1]

# Compute ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_prob, pos_label='yes')
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='green', lw=2, label=f'Decision Tree ROC (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Decision Tree Classifier')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot a simplified version of the tree for readability (max depth = 3)
plt.figure(figsize=(20, 10))
plot_tree(
    clf,
    filled=True,
    feature_names=X.columns,
    class_names=['no', 'yes'],
    rounded=True,
    max_depth=3,   # Limit tree depth for easier visualization
    fontsize=10
)
plt.title("Decision Tree Visualization (Max Depth = 3)")
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Use 'Heart Disease Status' instead of 'target' for both X and y
X = df.drop('Heart Disease Status', axis=1)
y = df['Heart Disease Status']

# One-hot encode categorical features before scaling
X = pd.get_dummies(X, columns=categorical_cols[:-1]) # Exclude 'Heart Disease Status' from encoding

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X) # Now apply scaling to the encoded data

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

svm_model = SVC(kernel='rbf', random_state=42)  # You can try 'linear', 'poly', etc.
svm_model.fit(X_train, y_train)

y_pred = svm_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of SVM: {accuracy:.2f}")

from sklearn.metrics import roc_curve, auc

svm_model = SVC(kernel='rbf', probability=True, random_state=42)
svm_model.fit(X_train, y_train)


# Get probability scores
y_prob = svm_model.predict_proba(X_test)[:, 1]

# Compute ROC
fpr, tpr, thresholds = roc_curve(y_test, y_prob, pos_label='yes')
roc_auc = auc(fpr, tpr)

# Plot ROC
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for SVM')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

# Reduce to 2 components for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Retrain SVM on reduced data for plotting only
svm_vis = SVC(kernel='rbf')
svm_vis.fit(X_pca, y)

# Plot
import seaborn as sns
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='coolwarm', alpha=0.7)
plt.title("PCA Visualization of Heart Disease Data (2D)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# Use 'Heart Disease Status' instead of 'target' for both X and y
X = df.drop('Heart Disease Status', axis=1)
y = df['Heart Disease Status']

# One-hot encode categorical features before scaling
X = pd.get_dummies(X, columns=categorical_cols[:-1])  # Exclude 'Heart Disease Status' from encoding


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)
mlp.fit(X_train, y_train)

y_pred = mlp.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of Neural Network (MLP): {accuracy:.2f}")

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Get predicted probabilities
y_prob = mlp.predict_proba(X_test)[:, 1]

# Compute ROC and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_prob, pos_label='yes')
roc_auc = auc(fpr, tpr)

# Plot
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='purple', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Neural Network (MLP)')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()

#Learning Curve / Loss Curve
plt.plot(mlp.loss_curve_)
plt.title('Training Loss Curve')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid(True)
plt.tight_layout()
plt.show()